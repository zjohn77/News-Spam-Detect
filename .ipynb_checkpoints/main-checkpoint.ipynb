{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buckshot++: A Scalable, Noise-Tolerant Bagging Approach to Clustering. (Inspired by the [Buckshot Algorithm](https://pdfs.semanticscholar.org/1134/3448f8a817fa391e3a7897a95f975ad2873a.pdf).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we unveil a new algorithm that improves upon the k-means by dealing with the main shortcoming of k-means, namely, the need to predetermine the number of clusters (which we'll call K). Typically, K is found in the following manner: \n",
    "1. settle on some metric,\n",
    "2. evaluate that metric at multiple values of K, \n",
    "3. use a greedy stopping rule to determine when to stop (typically the bend in an elbow curve)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There must be a better way. We detail the following 3 improvements that the Buckshot++ algorithm makes to k-means.   \n",
    "1. Not all metrics are create equal. K-means doesn't prescribe which metric to use for finding K. We analyze below that some commonly implemented metrics are too inconsistent to be useful. Buckshot++ prescribes the silhouette score for finding K.\n",
    "2. Another problem with k-means is that every single point is clustered--but what we really want is the pattern and not the noise. We show here a beautiful way to solve this problem -- a more elegant way than k-medoids and k-medians. \n",
    "3. Finally, the computational complexity of running k-means multiple times to find the best K can be prohibitive. We show a surprisingly simple alternative that scales better asymptotically. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The details of the Buckshot++ algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ALGORITHM**: Buckshot++ <br>\n",
    "**INPUTS**: population of *N* vectors <br>\n",
    "*B* := number of bootstrap samples <br>\n",
    "*F* := max number of clusters to try <br>\n",
    "*M* := cluster quality metric <br>\n",
    "**OUTPUT**: the optimal *K* for kmeans\n",
    "\n",
    "Take *B* bootstrap samples where each sample is of size 1/*B*.  \n",
    "**for each** counter *k* from 2 to *F* **do** <br>\n",
    "&emsp;&emsp;Compute kmeans with *k* centers. <br>\n",
    "&emsp;&emsp;Compute the metric *M* on the clusters. <br>\n",
    "Compute the centroid of all metrics vectors.  \n",
    "Get argmax of the centroid vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of the Buckshot++ algorithm\n",
    "The main idea of the Buckshot++ algorithm is motivated by the [Buckshot algorithm](https://pdfs.semanticscholar.org/1134/3448f8a817fa391e3a7897a95f975ad2873a.pdf), which essentially finds cluster centers by performing hierarchical clustering on a sample and then performs k-means by taking those cluster centers as input. Here's why the Buckshot algorithm takes this approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k-means</th>\n",
       "      <th>hierarchical</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Computational Complexity</th>\n",
       "      <td>O(N * k * d * i)</td>\n",
       "      <td>O(N^2 * logN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sources of Instability</th>\n",
       "      <td>random initial means; local minimum; outlier</td>\n",
       "      <td>outlier</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               k-means  \\\n",
       "Computational Complexity                              O(N * k * d * i)   \n",
       "Sources of Instability    random initial means; local minimum; outlier   \n",
       "\n",
       "                           hierarchical  \n",
       "Computational Complexity  O(N^2 * logN)  \n",
       "Sources of Instability          outlier  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbl = DataFrame({'k-means': ['O(N * k * d * i)', 'random initial means; local minimum; outlier'],\n",
    "                 'hierarchical': ['O(N^2 * logN)', 'outlier']}\n",
    "                , index=['Computational Complexity', 'Sources of Instability'])\n",
    "tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The big deal here is the high [computational complexity of hierarchical clustering](https://nlp.stanford.edu/IR-book/html/htmledition/time-complexity-of-hac-1.html), which is why the Buckshot algorithm performs hierarchical only on a sample. So in a nutshell, hierarchical is more deterministic/stable but less scalable than kmeans. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buckshot++'s key innovation lies in the step \"Take B [bootstrap samples](http://www.stat.rutgers.edu/home/mxie/rcpapers/bootstrap.pdf) where each sample is of size 1/B.\" So Buckshot is doing hierarchical on a sample, while Buckshot++ is doing multiple kmeans on *bootstrap* samples. What's neat is that bootstrapping takes care of scalability and stability simultaneously. Doing kmeans many times can still finish sooner than doing hierarchical just once, as the time complexities above show. In terms of stability, bootstrapping is a great way to smooth out noise. In fact, that is exactly why **Bagging** (a.k.a. **B**ootstrap **Agg**regat**ing**) and [Random Forests](https://en.wikipedia.org/wiki/Random_forest#From_bagging_to_random_forests) work so well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see Buckshot++ in code:\n",
    "Buckshot++ is implemented below. The data we use for illustration is the [Kaggle News Aggregator Dataset](https://www.kaggle.com/uciml/news-aggregator-dataset). In this data, there's a field that contains news headlines and another field that holds an ID indicating which story a headline belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'plot_mult_samples'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-ba07799597b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbuckshotpp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClusterings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_mult_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mchoice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'plot_mult_samples'"
     ]
    }
   ],
   "source": [
    "from buckshotpp import Clusterings, plot_mult_samples\n",
    "\n",
    "from numpy.random import choice\n",
    "from pandas import read_csv, DataFrame\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "import nltk; nltk.download('punkt')\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "# Pass in settings to instantiate a Clusterings object called vecSpaceMod1:\n",
    "vecSpaceMod = Clusterings({'file_loc': 'data/news_headlines.csv',\n",
    "                           'tf_dampen': True,\n",
    "                           'common_word_pct': 1,\n",
    "                           'rare_word_pct': 1,\n",
    "                           'dim_redu': False}\n",
    "                         )\n",
    "news_df = vecSpaceMod.get_file() # Read CSV input to data frame.\n",
    "metrics_byK = vecSpaceMod.buckshot(news_df)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interesting revelation from bootstrap\n",
    "Each green curve is generated from a bootstrap sample, and the red curve is their average. Remember the sources of instability for k-means listed in the table above? Outlier is one. The concept of outlier has somewhat different meaning in the context of clustering. In supervised learning, an outlier is a rare observation that's far from other observations distance-wise. In clustering, a far away observation is its own well-separated cluster. So my interpretation is that \"rare\" is the operative word here and that outliers are singleton clusters that exert undue influence on the formation of other clusters. Look at how [bagging](https://en.wikipedia.org/wiki/Bootstrap_aggregating) imparted a more stable estimate of the relationship between the number of clusters and the silhouette score in the graph below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_mult_samples(metrics_byK, 'silhouette')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful and not-so-useful internal clustering metrics currently out there.\n",
    "The two internal clustering metrics implemented in scikit-learn are: the Silhouette Coefficient and the Calinski-Harabasz criterion. Comparing the Silhouette plotted above with the Calinski plotted below, it's clear that Silhouette is clearly better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_mult_samples(metrics_byK, 'calinski')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = vecSpaceMod.term_weight_matr(news_df.TITLE)\n",
    "kmeans_fit = KMeans(20).fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are the quality of the clusters good?\n",
    "Taking a look at the documents and their corresponding \"predictedCluster\", the results certainly do impress!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_results = DataFrame({'predictedCluster': kmeans_fit.labels_,\n",
    "                           'document': news_df.TITLE})\n",
    "cluster_results.sort_values(by='predictedCluster', inplace=True)\n",
    "cluster_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Internal or External Clustering Metrics?\n",
    "With the ground truth labels, we can evaluate Mutual Information, which is the most commonly used external metric. Looking at 0.67 Mutual Information compared with about 0.35 Silhouette, the relationship between the metrics is fairly consistent with other studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mutual_info = adjusted_mutual_info_score(labels_true=news_df.STORY, labels_pred=kmeans_fit.labels_) \n",
    "print(mutual_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Buckshot++ Scales Better Than Buckshot.\n",
    "The fact is that hierarchical clustering has a much higher order of time complexity than k-means. This means that, for sizable inputs, running k-means multiple times is still faster than running hierarchical clustering just once. The Buckshot algorithm runs hierarchical just once on a small sample in order to initialize cluster centers for k-means. Since O(N^2 * logN) grows really fast, the sample must be really small to make it work computationally. But a key critique of Buckshot is [failure to find the right structure with a small sample](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.136.7906&rep=rep1&type=pdf).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our conclusion & the key advantages of Buckshot++\n",
    "* **Extremely accurate** method of estimating the number of clusters (a clearly best Silhouette emerged every time, while typical elbow heuristic searches can hit or miss).\n",
    "* **Highly scalable** (faster search for K achieved by using k-means rather than hierarchical; running k-means on subsample rather than everything).\n",
    "* **Robust to noise** when used in conjunction with k-means++ (sampling with replacement lessens the chance of selecting an outlier in the bootstrap sample)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
